### Why Language Models Hallucinate 梳理

#### Abstract
大模型幻觉的原因：大型语言模型有时会在不确定的情况下进行猜测，产生看似合理但不正确的陈述，而不是承认不确定性。
我们认为语言模型产生幻觉是因为训练和评估程序奖励猜测而不是承认不确定性，我们分析了现代训练流程中幻觉的统计原因。

> 补充知识点 pipeline:流水线。一个基础的深度学习的Pipeline主要包含了下述5个步骤：1.数据读取 2.数据预处理 3.创建模型 4.评估模型结果 5.模型调参

幻觉只是源于二元分类中的错误。
如果错误的陈述无法与事实区分开来，那么预训练语言模型中的幻觉将通过自然统计压力产生。
> 补充知识点：natural statistical pressures（自然的统计压力）：即使我们在预训练时目标函数（比如交叉熵）完全正确、训练数据里也没有错误，语言模型依然会不可避免地产生幻觉。这是由统计学习本身的限制导致的，而不是实现或训练技巧上的偶然问题。换句话说，它强调了一种“天然存在的数学必然性”。
1.有限数据带来的压力：数据太少，模型只能去推理没见过的情况，所以会错
2.目标函数带来的压力：交叉熵训练只能保证平均上概率逼近真实分布，不是每个都预测正确，这样会在一些低频的事实犯错
3.评价方式带来的压力：评测时常常是二元对错，模型即使“不知道”，硬猜一个也比老老实实说“我不知道”得分高。这种奖励机制就进一步推动模型“猜”，也就是幻觉。
修改现有基准的评分,而不是引入额外的幻觉评估。

> 补充知识点:IIV（Is-It-Valid）二分类问题

#### Introduction 
一般误差集E，一个看似合理的字符串X=E∪V的任意子集，而其他看似合理的字符串V被称为有效。
通用的错误分析：它只考虑了现代训练范式的两个阶段pretraining and post-training
对于幻觉，分类法通常会进一步区分与用户提示相矛盾的内在幻觉，我们的理论还揭示了与训练数据或外部现实相矛盾的外在幻觉。
> 补充知识点：内在幻觉（intrinsic hallucination）：生成内容与 用户提示本身的上下文 相矛盾。
外在幻觉（extrinsic hallucination）：生成内容与 训练数据中记录的事实 或 外部真实世界 相矛盾。
#### Errors caused by pretraining
即使使用无误差的训练数据，在预训练期间最小化的统计目标也会导致产生错误的语言模型。

有些模型不会出错：总是输出I don't know(IDK)，或者只是把语料库里的东西背下来。
Spelling分类器就很好，模型本身能力不足，counting分类器表现的就很差。有些事实是任意的，没什么规律，所以只能硬背。所以这样分类器就炸了。因此产生了幻觉。

一种任意事实的特殊情况，即数据中没有可学习的模式
##### 单例事实（只出现一次的事实）”与幻觉率之间的必然联系
作者前面已经把 生成是否正确 转换成 IIV（Is-It-Valid）二分类问题来分析。  
预训练时的幻觉率 ≥ 训练集中那些“只出现一次的事实”所占比例。  

事实上，包括提示和 IDK 反应，这两个都是幻觉的重要组成部分。（因为很多 benchmark 不允许“IDK”，反而逼着模型去幻觉）
##### 这段话的意思是:在训练数据中，只出现一次的事实会直接导致幻觉下界。

#### Why hallucinations survive post-training
简单地说，大多数评估都不一致。他们总是处于“应试”模式。  

我们观察到，现有的主要评估在很大程度上惩罚了不确定性，因此根本问题是大量评估不一致。

目前(2025.9.12)现象：造成了一种惩罚不确定性和弃权的“流行病”。

#### Pretraining Errors
##### 预训练基础语言模型的工作原理
预训练产生一个基础语言模型pˆ，该模型近似于从其训练分布p中提取的分布文本。

证明基础模型错误的关键挑战是许多语言模型没有错误。总是输出IDK的退化模型也避免了错误（假设IDK不是错误）。从随机训练示例中反刍文本的平凡基础模型也不会出错。
然而，这两种语言模型在密度估计方面失败了。

> 反刍(chú)文本（textual ruminations）: 模型在生成文本时“反复”生成与已有内容相似的内容。
幻觉与反刍：幻觉（hallucination） 是一个常见问题，模型可能“胡乱”生成看似真实但实际上不符合真实世界的内容。反刍文本有时也会加剧这一问题，因为模型可能会在没有依据的情况下反复生成错误的内容。

生成有效输出（即避免错误）比对输出有效性进行分类更难。
##### 很经典的两个方向：找对的 还是  避免错的
我们的分析表明，错误源于模型适合底层语言分布的事实，尽管特定的架构可能会引入额外的错误。
##### 基本模型pˆ的错误率：
\[
\text{err} := \hat{p}(E) = \Pr_{x \sim \hat{p}}[x \in E]
\]
##### IIV（二元分类）问题：
模型需要区分有效（𝑉）和错误（E）的样本：
\[
D(x) :=
\begin{cases}
\frac{p(x)}{2} & \text{if } x \in V \\
\frac{1}{2|E|} & \text{if } x \in E
\end{cases}
\]
##### IIV 分类函数：
分类函数$f(x)$将样本$x$映射为+1(有效)和-1（错误）：
\[
f(x) :=
\begin{cases}
+1 & \text{if } x \in V \\
-1 & \text{if } x \in E
\end{cases}
\]
##### IIV 误分类率：
\[
\text{err}_{\text{IIV}} := \Pr_{x \sim D}[f̂(x) \neq f(x)]
\]
##### 基础模型错误率与 IIV 错误率的关系：
\[
\text{err} \geq 2 \cdot \text{err}_{\text{IIV}} - \frac{|V|}{|E|} - \delta
\]

#### Discussion and limitations
由于幻觉的多面性，该领域很难就如何定义、评估和减少幻觉达成一致。为了简单起见，统计框架必须优先考虑某些方面，忽略其他方面
关于本文所用框架的范围和局限性，有几点需要注意。
- Plausibility and nonsense
- Open-ended generations
为简单起见，本文中给出的例子都是针对一个事实问题。然而，幻觉经常出现在开放式提示中，
- Search (and reasoning) are not panaceas
当搜索无法产生可靠的答案时，二进制评分系统本身仍然会奖励猜测。此外，搜索可能无助于计算错误，例如在字母计数示例中，或其他固有的幻觉。
- Latent context
扩展模型以允许“隐藏上下文”是很有趣的，这些上下文不是语言模型提示的一部分，但可用于判断与任意不确定性相关的错误。
- A false trichotomy
我们的形式主义不区分不同程度或不确定性的误差。显然，正确/不正确/IDK类别也是不完整的。
#### Conclusions
本文揭示了现代语言模型中的幻觉，从训练前的起源到训练后的持续存在。

我们认为大多数主流评估都会奖励幻觉行为。
对主流评估的简单修改可以重新调整激励措施，奖励适当的不确定性表达，而不是惩罚它们。