### Proximal Policy Optimization Algorithms
ç›¸å¯¹æ¥è¯´æ¯”è¾ƒç®€å•çš„ä¸€ç¯‡æ–‡ç« 

### motivation + å›é¡¾TRPOè®ºæ–‡çš„ä¸€äº›ç»“è®ºå’Œæ¨ç†è¿‡ç¨‹
TRPOçš„æå‡ºä¸»è¦æ˜¯è§£å†³deep Q-learningçš„ä¸€äº›ç¼ºç‚¹(è®­ç»ƒéå¸¸ä¸ç¨³å®š,è®­ç»ƒçš„æ—¶é—´éå¸¸é•¿ä½†æ˜¯performanceä¸èƒ½å¾—åˆ°ä¿è¯)
TRPOæŠŠä¼˜åŒ–å¼•å…¥äº†æ·±åº¦å¼ºåŒ–å­¦ä¹ å½“ä¸­.å¯¹äºå½“å‰æ‰§è¡Œçš„policyÎ 1é€šè¿‡é‡‡æ ·ç”Ÿæˆä¸€ä¸ªä¼˜åŒ–é—®é¢˜optimization1.é€šè¿‡è§£optimizationä¿è¯ä¸€å®šèƒ½å¤Ÿç”Ÿæˆæ›´å¥½çš„Î 2,performanceæ˜¯æœ‰æå‡çš„.å¦‚æ­¤ä¸æ–­åœ°å¾ªç¯å¾€å¤.


ç†è§£TRPOå¯¹äºç†è§£PPOæ˜¯è‡³å…³é‡è¦çš„
TRPOæ˜¯ä¸€ç§Policy Gradientçš„æ–¹æ³•.
$\nabla$Î¸ = $\hat{g}$ =ç¬¬tæ­¥çš„ç§¯åˆ†æ²¡çœ‹æ‡‚çš„å…¬å¼
ç”±äºå­˜åœ¨è‡ªåŠ¨æ±‚å¯¼ï¼Œä¸Šé¢çš„å¼å­ç­‰ä»·äºä¸€ä¸ªç›®æ ‡.ä¹Ÿæ˜¯ä¸€ä¸ªå…¬å¼
è¿™æ ·åšç†è®ºä¸Šæ²¡é—®é¢˜ï¼Œä½†æ˜¯ç°å®ä¸­ä¼šé‡åˆ°é—®é¢˜:ä¸‹ä¸€ä¸ªÎ¸'å’ŒåŸæ¥çš„Î¸çš„ç­–ç•¥å·®çš„éå¸¸å¤š(æœ‰é—®é¢˜,ä»–è¯´å’ŒåŸæ¥æ¯”éå¸¸å·®,ä½†æ˜¯æ€ä¹ˆçŸ¥é“åˆšå¼€å§‹åšçš„performanceå°±æ˜¯å¥½çš„?é‚£ä¸æ˜¯å…ˆå…¥ä¸ºä¸»çš„é”™è¯¯?)
å› æ­¤æå‡ºäº†TRPOæ–¹æ³•,æå‡ºäº†ä¸€ç§æ–¹æ³•:æŠŠæ–°è€policyçš„KL divergencyä½œä¸ºçº¦æŸé¡¹(è®©è¿™ä¸¤ä¸ªpolicyä¸è¦å·®çš„å¤ªå¤š),åŒæ—¶maximizeä¼˜åŒ–ç›®æ ‡.åœ¨TRPOè¿™ç¯‡è®ºæ–‡é‡Œä¹Ÿæåˆ°äº†ä¸€äº›NConstrainçš„ä¸€ç§æ–¹æ³•,å°†KL divergencyä½œä¸ºä¼˜åŒ–ç›®æ ‡çš„ä¸€éƒ¨åˆ†,åŸæ¥çš„TRPOè®­ç»ƒæ—¶é—´éå¸¸é•¿ï¼Œè½¬æ¢åˆ°NConstrainï¼Œæ•´ä¸ªçš„è®­ç»ƒæ—¶é—´å°±ä¼šéå¸¸å¿«.
PPOæœ¬è´¨ä¸Šå°±æ˜¯è¿™ä¸ªæ€è·¯çš„ä¸€ä¸ªå…·ä½“åŒ–,PPOè¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸¤ä¸ªæ–¹æ³•.

### approaché‡ç‚¹
ä¸€ç§æ–¹æ³•æ˜¯section3çš„Clipped Surrogate Objective,å¦ä¸€ç§æ–¹æ³•æ˜¯section4çš„Adaptive KL Penalty.
è¿™ä¸¤ç§æ–¹æ³•åœ¨æœ€åæ¯”è¾ƒä¹‹åClipped Surrogate Objectiveæ˜¯æ¯”è¾ƒå¥½çš„.

Clipped Surrogate Objectiveæ˜¯æ€ä¹ˆå·¥ä½œçš„?
ä¸Šå¼è¦ä¼˜åŒ–çš„ç›®æ ‡è®°ä¸º$L^{CPI}(Î¸)$ = ä¸¤ä¸ªpolicyçš„ratioä¹˜advantage-function  åœ¨tæ­¥çš„ç§¯åˆ†.åŒæ—¶Clipped Surrogate Objectiveå¯ä»¥å†™æˆä¸‹é¢çš„å¼å­(æ¯”è¾ƒå¤æ‚)

Adaptive KL Penalty:
è¿™ä¸ªæ–¹æ³•æ¯”è¾ƒç®€æ´,åˆå§‹çš„æå‡ºåœ¨TRPOæ–‡ç« é‡Œé¢.

---

ä¸Šé¢åšçš„ç¬”è®°æœ‰äº›æ™¦æ¶©ï¼Œæ‰€ä»¥é‡æ–°æ¢³ç†äº†ä¸€é

## æ¢³ç†

[æ— éœ€RLåŸºç¡€ç†è§£ PPO å’Œ GRPO](https://zhuanlan.zhihu.com/p/27704969958)ğŸ‘ˆçœ‹è¿™ä¸ªå¯ä»¥è¡¥è¡¥PPOï¼ŒGRPOåŸºç¡€


#### PPO ä¾èµ–äº Actor + Critic + è£å‰ª + KL æƒ©ç½šæ¡†æ¶ã€‚
**å¼•å…¥Critic**ï¼šä½¿ç”¨â€œé¢„æµ‹åˆ†æ•°çº¿â€æ¥æ”¹è¿›å¥–åŠ±
åœ¨ RL ä¸­ï¼Œè¿™ä¸ªâ€œåˆ†æ•°çº¿â€è¢«ç§°ä¸ºä»·å€¼å‡½æ•°ã€‚å®ƒå……å½“ä¸€ä¸ªåŸºå‡†ã€‚æˆ‘ä»¬çš„è®­ç»ƒç›®æ ‡ä»ä»…å¥–åŠ±è½¬å˜ä¸ºè¶…è¿‡è¯¥åŸºå‡†çš„ç¨‹åº¦ï¼Œç”±ä¼˜åŠ¿å‡½æ•°è¡¨ç¤ºã€‚

**æ·»åŠ è£å‰ªå’Œæœ€å°å€¼æ“ä½œ**ï¼šé˜²æ­¢è¿‡åº¦æ›´æ–°ã€‚åœ¨PPOï¼ˆè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼‰ä¸­ï¼Œè¿™ç§å¹³è¡¡é€šè¿‡â€œè£å‰ªâ€æœºåˆ¶å®ç°ã€‚é€šä¿—ç‚¹è¯´å°±æ˜¯è®¾ç½®ä¸Šé™ä¸‹é™ã€‚

**å‚è€ƒæ¨¡å‹**ï¼šé˜²æ­¢ä½œå¼Šå’Œæç«¯ç­–ç•¥ã€‚åœ¨å¤§è¯­è¨€æ¨¡å‹é¢†åŸŸï¼Œç±»ä¼¼çš„æƒ…å†µæ˜¯ç”Ÿæˆæœ‰å®³æˆ–è™šå‡å†…å®¹ä»¥äººä¸ºæé«˜æŸäº›å¥–åŠ±æŒ‡æ ‡ã€‚  


**GRPOï¼šç”¨â€œå¤šæ¬¡æ¨¡æ‹Ÿå¹³å‡å€¼â€æ›¿ä»£ä»·å€¼å‡½æ•°ã€‚** è¯¦è§GRPO

PPO-clip(ç”¨çš„æ¯”è¾ƒå¤š)  
å‰é¢æ±‚æœŸæœ›çš„éƒ¨åˆ†åŒ…å«äº†state,old probs,action,reward,next state.  
**ä»·å€¼value(critic)**:è¾“å…¥ä¸€ä¸ªçŠ¶æ€ï¼Œè¾“å‡ºä¸€ä¸ªä»·å€¼  
ä¸€å…±æœ‰3ä¸ªç½‘ç»œ:old policy,new policy(actor),value(critic)  
**ä¼˜åŠ¿å‡½æ•°ï¼ˆAdvantage Functionï¼‰** ç”¨æ¥è¡¡é‡åœ¨æŸä¸ªçŠ¶æ€ä¸‹ï¼Œé‡‡å–æŸä¸ªç‰¹å®šåŠ¨ä½œæ¯”â€œå¹³å‡è¡¨ç°â€å¥½å¤šå°‘ã€‚  

å¦‚ä½•åº”ç”¨åˆ°Large Language Model?  

**å› æœè¯­è¨€æ¨¡å‹Causal Language Model** :åé¢çš„å­—èƒ½çœ‹åˆ°å‰é¢çš„ï¼Œä½†æ˜¯å‰é¢çš„çœ‹ä¸åˆ°åé¢çš„(**è‡ªå›å½’**)  

è¯è¡¨logité‚£é‡Œåˆ†ç±»  

å¦‚ä½•è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹?  
1.æ”¶é›†é—®é¢˜ï¼Œè®©å¤§è¯­è¨€æ¨¡å‹è¾“å‡ºç»“æœã€‚äººç±»è¿›è¡Œæ’åº(æ’åºæ¯”æ‰“åˆ†å®¹æ˜“)  
2.è®­ç»ƒä¸€ä¸ªäººç±»åå¥½çš„å¥–åŠ±æ¨¡å‹(reward model),ç”¨æ¥ç»™å¤§è¯­è¨€æ¨¡å‹çš„è¾“å‡ºæ‰“åˆ†,ç”¨Bradley-Terry(ä¸»è¦ç”¨äºä½“è‚²èµ›äº‹)ç®—æ³•è®¡ç®—loss    
3.Train policy with PPO  

é”™ä½ç°è±¡  
å…·ä½“çš„è¿˜æ˜¯çœ‹å›¾è§£å§  

---

## å†æ¢³ç†

#### æ ‡å‡†ç‰ˆçš„ç­–ç•¥æ¢¯åº¦ç®—æ³•(Vanilla Policy Gradient)
[æ ‡å‡†ç‰ˆçš„ç­–ç•¥æ¢¯åº¦ç®—æ³•(Vanilla Policy Gradient)](https://zhuanlan.zhihu.com/p/106006748)ğŸ‘ˆç‚¹è¿™
[ä»å¤´ç†è§£PPO(Proximal Policy Optimization)ï¼šä»å…¬å¼åˆ°ä»£ç ](https://zhuanlan.zhihu.com/p/3333839684)ğŸ‘ˆç‚¹è¿™

> å†·çŸ¥è¯†: è®¡ç®—æœºé¢†åŸŸç‰¹æŒ‡æœªç»ä¿®æ”¹çš„åŸç”Ÿç‰ˆæœ¬è½¯ä»¶ã€‚ç”±äºé¦™è‰å¸¸ä½œä¸ºå†°æ·‡æ·‹åŸºç¡€å£å‘³ï¼Œvanillaå¼•ç”³å‡º"æ™®é€šçš„""é»˜è®¤çš„"æ¯”å–»ä¹‰ã€‚

> æ­£å› å¦‚æ­¤ï¼Œæˆ‘ä»¬ç»å¸¸åœ¨ç¿»è¯‘é‡Œé¢çœ‹åˆ°â€œé¦™è‰â€ç­–ç•¥æ¢¯åº¦æ–¹æ³•ã€‚

ç­–ç•¥æ¢¯åº¦(Policy Gradient,PG)æ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³:å›æŠ¥é«˜çš„åŠ¨ä½œé‡‡æ ·æ¦‚ç‡ä¸æ–­æé«˜ï¼Œå›æŠ¥ä½çš„åŠ¨ä½œé‡‡æ ·æ¦‚ç‡ä¸æ–­é™ä½ï¼Œä»è€Œè¾¾åˆ°æœ€ä¼˜ç­–ç•¥ã€‚

æ ‡å‡†çš„ç­–ç•¥æ¢¯åº¦ç®—æ³•(Vanilla Policy Gradient, VPG)å±äºåœ¨ç­–ç•¥(on-policy)ç®—æ³•ã€‚

> The theory justifying TRPO actually suggests using a penalty instead of a constraint

è¿™å¥è¯å¥½åƒåé¢æœ‰æ–‡ç« åšäº†ï¼Œåé¢ä¼šå›æ¥è¡¥å……ä¸€ä¸‹ã€‚

> This objective can further be augmented by adding an entropy bonus to ensure sufficient exploration, as suggested in past work [Wil92; Mni+16].

è¿™å¥è¯æ˜¯è¯´å¢åŠ æ¢ç´¢èƒ½åŠ›çš„æ–¹æ³•

##### å¯¹æ•°ç©ºé—´è¿™ä¸ªç‚¹å¯ä»¥æ‰¾æ—¶é—´çœ‹ä¸€ä¸‹ï¼Œæ„Ÿè§‰å€¼å¾—ç ”ç©¶
ä¸ºä»€ä¹ˆï¼Ÿæ€ä¹ˆåšï¼Ÿæ€ä¹ˆæ¥çš„ï¼Ÿè¯æ˜ï¼Ÿåº”ç”¨èŒƒå›´ï¼Ÿ