# DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models  
专注于数学推理  
GRPO放弃了批评模型，而是根据小组得分估算基线，从而显著减少了培训资源。  
参数数量不是数学推理能力的唯一关键因素，在高质量数据上预先训练的较小模型也可以实现强大的性能。  
## 首先他数据洗得好  
这块我们不细看  
## GRPO  
组相对策略优化强化学习（RL）已被证明在监督微调（SFT）阶段后能有效地进一步提高LLM的数学推理能力。本文，我们将介绍我们高效且有效的RL算法——组相对策略优化（GRPO）。    
在PPO中，需要与策略模型一起训练价值函数，并减轻奖励模型的过度优化，标准方法是在每个代币的奖励中添加来自参考模型的每个代币KL惩罚  
由于PPO中使用的值函数通常是与策略模型大小相当的另一个模型，因此它带来了大量的内存和计算负担。此外，在RL训练期间，值函数被视为计算方差减少优势的基线。而在LLM环境中，通常只有最后一个令牌被奖励模型分配奖励分数，这可能会使在每个令牌上准确的值函数的训练复杂化。为了解决这个问题，如图4所示，我们提出了组相对策略优化（GRPO），它消除了在PPO中对额外值函数近似的需要，而是使用响应同一问题而产生的多个采样输出的平均奖励作为基线。  
### 减少了内存，换了一种方式作为基线  
GRPO没有在奖励中添加KL惩罚，而是通过将训练策略和参考策略之间的KL分歧直接添加到损失中来进行正则化，避免使Aˆi，t的计算复杂化。  
在迭代GRPO中，我们根据策略模型的采样结果为奖励模型生成新的训练集，并使用包含10%历史数据的重放机制不断训练旧的奖励模型。  
## Limitation  
尽管DeepSeekMath在定量推理基准测试中取得了令人印象深刻的成绩，但它在几何和定理证明方面的能力相对弱于封闭模型。