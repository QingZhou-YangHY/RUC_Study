# An Empirical Study on Eliciting and Improving R1-like Reasoning Models  
##### 这篇paper/报告很好，有很多知识点可以学  
本技术报告旨在记录从大量强化学习实验中获得的观察结果和见解。具体来说，在本报告中，我们首先深入研究了强化学习设置对训练效果的影响。接下来，我们通过强化学习训练直接激励基础模型发展复杂的推理能力，观察到该模型逐渐花费3美元。一个重要且有前景的研究方向是研究开发更具原则性的强化学习（RL）训练方法，利用一般领域奖励来解决复杂的推理任务。我们把这个具有挑战性的话题留给未来的工作。 

- 大型推理模型的性能深受RL设置的影响。通过测试一系列参数配置来彻底调查和记录这些影响（表1）。值得注意的是，政策学习策略被证明是一个关键因素，使模型能够在整个强化学习过程中实现一致的性能改进，并证明其对加强强化学习培训的重大影响（第3.1节）。  
- 经过预训练后，基础模型已经显示出执行单个复杂推理动作的潜力。强化学习过程有效地激活了这种能力，使模型能够将这些行为整合到连贯和深思熟虑的思维过程中。我们的强化学习训练方法持续改进QWEN2.5-32B基础模型，提高了响应长度和测试精度（第3.2节）。  
- 响应长度是RL培训成功的重要指标；然而，这是性能改进的结果，而不是原因。设计专门的奖励函数来明确鼓励模型产生更长的响应可能会导致奖励黑客攻击等问题，这不能从本质上增强模型的推理能力（第4.1节和第4.2节）。  
- 强化学习训练持续提高微调模型的性能，包括短和长CoT推理模型。即使在Qwen2.5-1.5B通过蒸馏数据训练达到高水平的性能后，强化学习训练也进一步提高了其能力，在AIME 2024上达到了39.33的显著准确率（第4.3节）。  
- 通过监督微调，LRM可以获得操纵外部工具的能力，从而显著提高模型的性能。通过有效利用工具操作，STILL-3-tool-32B在AIME 2024上实现了86.67（贪婪搜索）的惊人准确率。值得注意的是，这种能力只需少量高质量的训练实例即可激活（第4.4节）。  
## 一些学到的知识点  

### 训练框架
使用两个流行的开源存储库进行实验，用于语言模型的强化学习：OpenRLHF[10]和veRL[11]。为了在第4.3节中训练STILL-31.5B，我们利用OpenRLHF来实现主要代码。对于所有其他实验，包括第3节中的基础模型和第4节中的微调模型，我们采用veRL来开发我们的实验。  
### 奖励设计
设计并验证了一组不同的奖励，并分析了它们对模型性能的影响，包括输出奖励、格式奖励、长度奖励和动作奖励。首先，输出奖励评估最终答案是否与地面真相匹配。如果答案正确，我们将奖励设置为1，否则设置为0。此外，如果模型未能将其最终答案放入\boxed{}中，则奖励设置为0。其次，我们在第3节的“零”实验中使用格式奖励[9]来指导基础模型正确构建其响应。此外，我们在第4.2节中探索了新的辅助奖励，包括鼓励更长反应的长度奖励和激励复杂推理行为的行动奖励。  
传统的训练后方法通常依赖于SFT在RL训练前初始化模型，目的是为有效的RL优化建立一个坚实的基础。在DeepSeekR1 Zero[9]之后，在本节中，我们通过直接将RL应用于预训练的基础模型进行实验，而不需要任何中间的SFT阶段。这种方法旨在探索LLM是否可以通过纯粹的RL驱动的自我提升来自主发展推理能力。我们的实验系统地考察了四个关键维度。首先，我们实证研究了训练超参数的影响。其次，我们通过比较不同的基础模型并将其与具有短CoT推理能力的微调模型进行对标，来分析骨干模型的效果。第三，我们深入研究了快速设计对RL训练中基础模型推理能力的影响。最后，我们研究了RL训练过程中代表性推理模式（如验证或反射）的出现。此外，我们采用了我们对QWEN2.5-32B强化学习训练的研究结果，并提出了STILL-3-ZERO-32B，它在没有SFT过程的情况下进行直接强化学习训练。  
### 学习策略
政策内与政策外。我们主要探讨两种学习策略：政策内学习和政策外学习。在策略学习中，每次梯度更新都使用仅在当前策略模型的分布下生成的数据。相比之下，非政策学习利用的数据可能不是来自当前政策。如图1（b）所示，我们的实证结果表明，政策学习会产生更有利的结果。全面的政策培训方法鼓励更多的探索；在训练过程中，模型自然快速地增加了响应长度，而更新较少的非策略学习在长度增长方面遇到了瓶颈。此外，on policy方法在测试集上实现了卓越的性能，突显了该策略的独特优势。  
### 展开参数。
我们主要研究影响模型性能的两个卷展参数：卷展次数和卷展温度。推出次数（n）是指策略模型在RL过程中对每个查询的完整响应进行采样的次数，而推出温度（T）表示解码过程中使用的温度系数。更大的推出数量和更高的温度通常表示更大程度的探索。 
我们比较了n=8和n=64的情况，发现增加推出次数可以显著提高训练性能，鼓励模型通过更多的探索产生更长的响应。然而，更高的温度并不总是有益的；高温可以促进探索，但也可能导致产生更多无意义的内容（重复、胡言乱语等）。我们在图1（c）中进行了T值为0.6、1.0和1.2的比较实验。结果表明，在训练的早期阶段，较低的温度会产生较高的平均回报，但这可能会限制模型的后续探索，导致后期表现不佳。相反，在T=1.2时，该模型的响应长度迅速增长，在AIME 2024上优于其他设置（即T=1.0和T=0.6）。基于实验结果，我们得出结论，在模型可以正常生成文本而没有乱码的情况下，更高的温度往往会带来更好的性能。  
### KL惩罚系数  
在RL训练过程中，Kullback-Leibler散度应该施加约束，使更新的策略不会与之前收集数据的行为策略有显著偏差。KL惩罚系数的较大值会导致更强的约束，从而确保更新的策略更接近行为策略。因此，KL惩罚系数的选择对于平衡强化学习中探索性和稳定性之间的权衡至关重要。我们进行了三个关于KL系数的实验：一个没有KL惩罚（KL=0），一个具有固定KL值（KL=1×10−3），另一个具有动态KL退火（余弦从KL=1？0衰减到KL=0）。在图1（d）中，我们可以观察到动态KL退火策略比其他变体具有更好的性能。在训练的早期阶段，模型容易崩溃，并可能陷入局部最优。固定的KL值可以约束参数更新，防止模型退化。然而，随着训练的进行，较大的KL值变得不适合强化学习，限制了模型能力的进一步改进。通过逐步放宽这些约束，动态KL退火能够持续改进骨干模型。  
### STILL-3-ZERO-32B
强化学习提高了基础模型的推理能力。在本节中，我们的目的是研究如何通过强化学习训练将基础模型提升到高水平的复杂推理。根据我们之前的研究，我们采用了精心调整的训练设置，包括超参数、骨干模型和训练数据的选择。RL设置。正如我们早期的实验（第3.1节）所证明的那样，我们观察到，规模较小的基础模型在拥有复杂的推理能力方面往往效果较差。因此，在本节中，我们采用QWEN2.5-32B作为骨干模型进行进一步的实验。考虑到有效性和计算资源的消耗，我们将训练批大小设置为128，并推出16次，执行了一个基于策略的高效训练过程。我们采用1×10−6作为学习率，并关闭KL惩罚以消除参考模型施加的约束，并利用90k训练问题（第2节）。此外，为了进行上下文高效的训练过程，我们首先将最大上下文长度设置为8k，然后逐渐将其扩展到20k。  
由于基础模型在训练的早期阶段倾向于生成过长且无意义的文本（例如，重复文本或胡言乱语），因此设置较小的上下文长度可以提前停止这些无意义的内容，以提高训练效率。  
###### 真是这样吗？上下文还是长一点好吧  
### 自我反思
生成的回复中关键字类别的比例。自我验证类别包括“验证”、“双重检查”和“确认”等词。自我反思类别包含“然而”、“反思”和“等待”等术语。自我纠正类别包括“纠正”、“修改”和“调整”等词。  
### 冷启动Cold Start  
冷启动指的是当推荐系统面临一些新的或没有足够历史数据的情况下，如何进行推荐。
### 热启动Warm Start  
热启动是指推荐系统已经积累了足够的历史数据，可以有效地为用户生成个性化推荐的情况。在热启动阶段，推荐系统可以依赖用户的历史行为和交互数据来进行推荐，因为系统已经了解了用户的兴趣和喜好。  

热启动通常比冷启动更容易，因为有足够的数据来支持推荐算法的运行。  

##### 在冷启动情况下，系统需要处理缺乏历史数据的情况，而在热启动情况下，系统可以充分利用历史数据来进行推荐。解决冷启动问题是推荐系统领域的一个重要挑战  
### 长CoT训练数据构建  
构建长CoT训练数据有两种主要方法：合成长CoT数据和从大型推理模型中提取。对于第一种方法，我们综合了包含各种推理模式（如反射和验证）的指令调优数据，以鼓励探索和多样化推理模式。根据现有的工作[20]，我们从他们的公共数据集中选择了大约5000个难题。对于第二种方法，我们使用现有的大型推理模型进行蒸馏，如先前研究[8]所述。我们从OpenThoughts[21]中选择了大约50k个数据，该数据从DEEPSEK-R1中提取了长CoT响应。然后，我们进行后处理，以选择适当长度和精度的响应。在准备了这两种类型的长CoT训练数据后，我们分别对基础模型进行了冷启动和监督微调。  
###### 实验得出的小结论：合成数据通常是使用相对简单的策略生成的，这些策略不足以捕捉或表示从高能力推理模型中更有效地提取出来的复杂推理过程。  
###### 有时候不能光想，还得做实验看结果，比如之前的BERT就是，莫名其妙。
### RL中长度黑客攻击的探讨
较长的响应长度通常与较大的搜索空间相关，这可以增强LLM的推理能力。然而，我们实证观察到，LLM可以利用长度偏差的奖励函数（即奖励黑客），这些函数旨在鼓励更长的反应和更复杂的推理行为。
#### Question Selection (QS)

#### Response Selection (RS)
与上述问题选择策略类似，响应选择的主要目标是引导LLM学习生成更长的响应，这可以在推理过程中扩展搜索空间。在RL过程中，在获得问题的生成解后，我们检查其正确性并计算每个解的长度。如果最长正确解的长度超过最短错误解的长度，我们引导模型从这两个解中学习。通过这种方式，模型可以学习增加生成的解的长度。  
#### Rewards on Response Length (RRL)
感觉太武断了  
#### Rewards on Reasoning Action (RRA)  
#### Overlength Response Masking (ORM)
这位更是重量级
